{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[0,0],[0,1],[1,0],[1,1]]\n",
    "y_data = [[0],[1],[1],[0]]\n",
    "#이러한 문제를 위해 multilayer를 만드는 것이다.\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None,2])\n",
    "Y = tf.placeholder(tf.float32, shape = [None,1])\n",
    "\n",
    "# 지금까지  2,1 로 받았었지만 중간에 하나의 layer을 둘 것이고, output값으로 2개를 받는다는 뜻이다.\n",
    "# 입력값으로 바로 출력값을 내는 것이 아니라, layer를 하나 더 두고 출력하는 것이다.\n",
    "W1 = tf.Variable(tf.random_normal([2,2]))\n",
    "b1 = tf.Variable(tf.random_normal([2]))\n",
    "# 중간의 두 층을 연결하는 layer1\n",
    "# 첫번째 layer의 output값을 w2의 input값으로 넣는다.\n",
    "# 처음의 입력값을 더 세분화해서 정보를 더 많이 끌어내기 위해 layer를 확인한다.\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "# 중간 층의 layer를 받아서 output값 전의 layer\n",
    "W2 = tf.Variable(tf.random_normal([2,1]))\n",
    "b2 = tf.Variable(tf.random_normal([1]))\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "cost = -tf.reduce_sum(tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis) ))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "\n",
    "# wide하게 한다 -- 노드수를 늘린다.\n",
    "# W1 = tf.Variable(tf.random_normal([2,10]))\n",
    "# b1 = tf.Variable(tf.random_normal([10]))\n",
    "# layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "# W2= tf.Variable(tf.random_normal([10,10]))\n",
    "# b2= tf.Variable(tf.random_normal([10]))\n",
    "# layer2= tf.sigmoid(tf.matmul(layer1,W2)+b1)\n",
    "\n",
    "# W3= tf.Variable(tf.random_normal([10,10]))\n",
    "# b3= tf.Variable(tf.random_normal([10]))\n",
    "# layer2= tf.sigmoid(tf.matmul(layer2,W3)+b1)\n",
    "\n",
    "# W4= tf.Variable(tf.random_normal([10,1]))\n",
    "# b4= tf.Variable(tf.random_normal([1]))\n",
    "# hypothesis = tf.sigmoid(tf.matmul(layer3,W4)+b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0 cost:  1.1870394\n",
      "step:  100 cost:  0.6792649\n",
      "step:  200 cost:  0.6715214\n",
      "step:  300 cost:  0.66391444\n",
      "step:  400 cost:  0.65543437\n",
      "step:  500 cost:  0.6457839\n",
      "step:  600 cost:  0.63498133\n",
      "step:  700 cost:  0.6232407\n",
      "step:  800 cost:  0.6108539\n",
      "step:  900 cost:  0.598074\n",
      "step:  1000 cost:  0.5850235\n",
      "step:  1100 cost:  0.5716341\n",
      "step:  1200 cost:  0.5576179\n",
      "step:  1300 cost:  0.5424556\n",
      "step:  1400 cost:  0.5253944\n",
      "step:  1500 cost:  0.5054686\n",
      "step:  1600 cost:  0.48160946\n",
      "step:  1700 cost:  0.4529347\n",
      "step:  1800 cost:  0.41923246\n",
      "step:  1900 cost:  0.38141024\n",
      "step:  2000 cost:  0.34150928\n",
      "step:  2100 cost:  0.30209786\n",
      "step:  2200 cost:  0.26540375\n",
      "step:  2300 cost:  0.23276749\n",
      "step:  2400 cost:  0.20462076\n",
      "step:  2500 cost:  0.1807718\n",
      "step:  2600 cost:  0.1607221\n",
      "step:  2700 cost:  0.14388601\n",
      "step:  2800 cost:  0.12970617\n",
      "step:  2900 cost:  0.11769893\n",
      "step:  3000 cost:  0.10746399\n",
      "step:  3100 cost:  0.09867754\n",
      "step:  3200 cost:  0.091080226\n",
      "step:  3300 cost:  0.08446502\n",
      "step:  3400 cost:  0.07866637\n",
      "step:  3500 cost:  0.07355146\n",
      "step:  3600 cost:  0.06901288\n",
      "step:  3700 cost:  0.06496364\n",
      "step:  3800 cost:  0.061332457\n",
      "step:  3900 cost:  0.05806072\n",
      "step:  4000 cost:  0.055099785\n",
      "step:  4100 cost:  0.052409187\n",
      "step:  4200 cost:  0.04995491\n",
      "step:  4300 cost:  0.047708213\n",
      "step:  4400 cost:  0.045644775\n",
      "step:  4500 cost:  0.043743767\n",
      "step:  4600 cost:  0.04198735\n",
      "step:  4700 cost:  0.040360074\n",
      "step:  4800 cost:  0.038848694\n",
      "step:  4900 cost:  0.037441503\n",
      "step:  5000 cost:  0.036128413\n",
      "step:  5100 cost:  0.03490054\n",
      "step:  5200 cost:  0.03375013\n",
      "step:  5300 cost:  0.032670163\n",
      "step:  5400 cost:  0.031654585\n",
      "step:  5500 cost:  0.030697862\n",
      "step:  5600 cost:  0.029795216\n",
      "step:  5700 cost:  0.028942248\n",
      "step:  5800 cost:  0.028135072\n",
      "step:  5900 cost:  0.027370129\n",
      "step:  6000 cost:  0.026644342\n",
      "step:  6100 cost:  0.025954766\n",
      "step:  6200 cost:  0.025298843\n",
      "step:  6300 cost:  0.024674188\n",
      "step:  6400 cost:  0.024078706\n",
      "step:  6500 cost:  0.023510402\n",
      "step:  6600 cost:  0.022967529\n",
      "step:  6700 cost:  0.02244841\n",
      "step:  6800 cost:  0.021951523\n",
      "step:  6900 cost:  0.021475542\n",
      "step:  7000 cost:  0.021019215\n",
      "step:  7100 cost:  0.02058129\n",
      "step:  7200 cost:  0.020160787\n",
      "step:  7300 cost:  0.019756615\n",
      "step:  7400 cost:  0.019367963\n",
      "step:  7500 cost:  0.01899387\n",
      "step:  7600 cost:  0.018633584\n",
      "step:  7700 cost:  0.018286366\n",
      "step:  7800 cost:  0.0179515\n",
      "step:  7900 cost:  0.01762841\n",
      "step:  8000 cost:  0.01731645\n",
      "step:  8100 cost:  0.017015044\n",
      "step:  8200 cost:  0.01672376\n",
      "step:  8300 cost:  0.016442008\n",
      "step:  8400 cost:  0.016169362\n",
      "step:  8500 cost:  0.015905386\n",
      "step:  8600 cost:  0.015649721\n",
      "step:  8700 cost:  0.015401987\n",
      "step:  8800 cost:  0.015161751\n",
      "step:  8900 cost:  0.014928721\n",
      "step:  9000 cost:  0.014702633\n",
      "step:  9100 cost:  0.01448312\n",
      "step:  9200 cost:  0.014269965\n",
      "step:  9300 cost:  0.014062831\n",
      "step:  9400 cost:  0.01386146\n",
      "step:  9500 cost:  0.013665693\n",
      "step:  9600 cost:  0.013475269\n",
      "step:  9700 cost:  0.013289902\n",
      "step:  9800 cost:  0.01310954\n",
      "step:  9900 cost:  0.012933895\n"
     ]
    }
   ],
   "source": [
    "sess=  tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(10000):\n",
    "    c_v  , _ = sess.run([cost,train],feed_dict = {X:x_data,Y:y_data})\n",
    "    if step % 100 == 0 :\n",
    "        print(\"step: \",step, \"cost: \", c_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "print(sess.run(accuracy,feed_dict = {X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
